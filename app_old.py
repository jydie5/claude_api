import streamlit as st
import streamlit_authenticator as stauth
import yaml
from yaml.loader import SafeLoader
import os
import base64
from openai import OpenAI
from audio_recorder_streamlit import audio_recorder
#from anthropic import AnthropicBedrock
import re
import anthropic

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY3"))

# AnthropicBedrockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
#anthropic_client = AnthropicBedrock()
anthropic_client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

#ãƒšãƒ¼ã‚¸åˆæœŸè¨­å®š
st.set_page_config(
    page_title="CLAUDE3_CHAT",
    page_icon="ğŸŒ",
    #layout="centered",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
def load_config():
    with open('.config.yaml') as file:
        return yaml.load(file, Loader=SafeLoader)

# èªè¨¼ã®åˆæœŸåŒ–
def init_authenticator(config):
    return stauth.Authenticate(
        config['credentials'],
        config['cookie']['name'],
        config['cookie']['key'],
        config['cookie']['expiry_days'],
    )

# éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°
def audio_to_text(audio_bytes):
    write_audio_file("recorded_audio.wav", audio_bytes)
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=open("recorded_audio.wav", mode="rb"),
    )
    return transcript.text

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€é–¢æ•°
def write_audio_file(file_path, audio_bytes):
    with open(file_path, "wb") as audio_file:
        audio_file.write(audio_bytes)

# # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã™ã‚‹é–¢æ•°
# def text_to_speech(text, voice):
#     response = client.audio.speech.create(
#         model="tts-1",
#         voice=voice,
#         input=text,
#         response_format="mp3"
#     )
#     audio_data = response.content
#     return audio_data
        
# ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã™ã‚‹é–¢æ•°
def text_to_speech(text, voice):
    response = client.audio.speech.create(
        model="tts-1",
        voice=voice,
        input=text,
        response_format="mp3"
    )
    audio_data = response.content
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ã«MP3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    with open("tts.mp3", "wb") as f:
        f.write(audio_data)
    
    return audio_data

# ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹é–¢æ•°
# def get_response(prompt, chat_history, output_container):
#     response = ""
#     with anthropic_client.messages.stream(
#         max_tokens=100000,
#         messages=[
#             *[
#                 {
#                     "role": message["role"],
#                     "content": [
#                         {
#                             "type": "text",
#                             "text": message["content"]
#                         }
#                     ]
#                 }
#                 for message in chat_history
#             ],
#             {
#                 "role": "user",
#                 "content": [
#                     {
#                         "type": "text",
#                         "text": prompt
#                     }
#                 ]
#             }
#         ],
#         model=st.session_state.model,  # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
#     ) as stream:
#         for text in stream.text_stream:
#             response += text
#             output_container.markdown(response)  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã‚’æ›´æ–°
#     return response



def get_response(prompt, chat_history, output_container):
    response = ""
    with anthropic_client.messages.stream(
        max_tokens=4096,
        messages=[
            *[
                {
                    "role": message["role"],
                    "content": message["content"]
                }
                for message in chat_history
            ],
            {
                "role": "user",
                "content": prompt
            }
        ],
        model=st.session_state.model,  # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    ) as stream:
        for text in stream.text_stream:
            response += text
            output_container.markdown(response)  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã‚’æ›´æ–°
    return response


# ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
def extract_topic(user_input, assistant_response):
    user_words = set(user_input.lower().split())
    assistant_words = set(assistant_response.lower().split())
    common_words = user_words.intersection(assistant_words)
    if common_words:
        topic = max(common_words, key=len)
        return re.sub(r'[^a-zA-Z0-9]+', '_', topic)  # ç‰¹æ®Šæ–‡å­—ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
    return None

# ä¼šè©±å±¥æ­´ã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°
def save_conversation_history(conversation_history, topic=None):
    if topic:
        file_name = f"{topic}.md"
    else:
        file_name = "conversation.md"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«å
    with open(file_name, "w") as file:
        for message in conversation_history:
            if message["role"] == "user":
                file.write(f"User: {message['content']}\n\n")
            else:
                file.write(f"Assistant: {message['content']}\n\n")
    return file_name

def main():
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    config = load_config()

    # èªè¨¼ã®åˆæœŸåŒ–
    authenticator = init_authenticator(config)

    # èªè¨¼ç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        authenticator.login()

    if st.session_state["authentication_status"]:
        st.sidebar.write(f'Login Name *{st.session_state["name"]}*')

        # ãƒ¡ã‚¤ãƒ³ç”»é¢ã§ã‚¢ãƒ—ãƒªã®å®Ÿè£…
        st.title('Anthropic Claude 2023.06-03-v1:0')

        # ã‚¢ãƒ—ãƒªç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼
        # ã‚¢ãƒ—ãƒªç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼
        with st.sidebar:
            st.markdown("## ãŠã—ã‚ƒã¹ã‚Šæ©Ÿèƒ½")
            if "enable_tts" not in st.session_state:
                st.session_state.enable_tts = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§TTSã‚’ç„¡åŠ¹åŒ–
            enable_tts = st.checkbox("Enable Text-to-Speech", st.session_state.enable_tts)
            st.session_state.enable_tts = enable_tts

            if "voice" not in st.session_state:
                st.session_state.voice = "nova"

            voice = st.selectbox("TTS Voice in OpenAI", ["alloy", "echo", "fable", "onyx", "nova", "shimmer"], index=4)
            st.session_state.voice = voice

            # ãƒ­ãƒ¼ã‚«ãƒ«ã®tts.mp3ã‚’å†ç”Ÿã™ã‚‹ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
            if os.path.exists("tts.mp3"):
                if st.button("ã‚‚ã†ä¸€åº¦ãŠã—ã‚ƒã¹ã‚Š"):
                    audio_file = open("tts.mp3", "rb")
                    audio_bytes = audio_file.read()
                    st.audio(audio_bytes, format="audio/mp3")

            # ãƒ¢ãƒ‡ãƒ«é¸æŠã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‚’è¿½åŠ 
            st.markdown("## ãƒ¢ãƒ‡ãƒ«é¸æŠ")
            model_options = {
                "haiku(ç´ æ—©ã„å—ã‘ç­”ãˆ)": "claude-3-haiku-20240307",
                "sonnet(ä¸€èˆ¬çš„AI)": "claude-3-sonnet-20240229",
                "opus(æœ€å¤§ãƒ¢ãƒ‡ãƒ«)":"claude-3-opus-20240229",
                "sonnet3.5(æœ€æ–°)": "claude-3-5-sonnet-20240620"
                
            }
            selected_model = st.selectbox("Claude3 Model Type", list(model_options.keys()))
            st.session_state.model = model_options[selected_model]

            # ä¼šè©±ã‚’ä¿å­˜ã™ã‚‹ãƒœã‚¿ãƒ³
            st.markdown("## ãƒ­ã‚°ä¿å­˜")
            if st.button("ä¼šè©±å±¥æ­´ã‚’ä¿å­˜"):
                topic = extract_topic(st.session_state.messages[-2]["content"], st.session_state.messages[-1]["content"])
                file_name = save_conversation_history(st.session_state.messages, topic)
                with open(file_name, "rb") as file:
                    st.download_button(
                        label="ä¼šè©±å±¥æ­´ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=file,
                        file_name=file_name,
                        mime="text/markdown"
                    )
            st.markdown("## ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ")  
                  
        authenticator.logout('Logout', 'sidebar')

    
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if "chat_session" not in st.session_state:
            st.session_state.chat_session = []
            st.session_state.messages = []
            st.session_state.enable_tts = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§TTSã‚’æœ‰åŠ¹åŒ–
        if "topics" not in st.session_state:
            st.session_state.topics = []
        


        # æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("New Chat"):
            st.session_state.chat_session = []
            st.session_state.messages = []
            st.session_state.topics = []

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
        if st.session_state.messages:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
        else:
            st.write("No conversation history.")

        # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
        input_text = st.chat_input("Type your message here...", key="input_text")

        # éŸ³å£°å…¥åŠ›
        audio_bytes = audio_recorder(
            text="<<<Recording ends,click again or remain silent for two seconds.>>>",
            recording_color="#e8b62c",
            neutral_color="#2EF218",
            icon_name="microphone-lines",
            icon_size="4x",
            pause_threshold=2.0,
            sample_rate=41_000
        )

        # å…¥åŠ›å‡¦ç†
        if input_text:
            st.session_state.messages.append({"role": "user", "content": input_text})
            with st.chat_message("user"):
                st.markdown(input_text)

            # éå»10å›åˆ†ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
            chat_history = st.session_state.chat_session[-20:]

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
            output_container = st.empty()

            full_response = get_response(input_text, chat_history, output_container)
            st.session_state.chat_session.append({"role": "user", "content": input_text})
            st.session_state.chat_session.append({"role": "assistant", "content": full_response})

            st.session_state.messages.append({"role": "assistant", "content": full_response})

            # ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
            topic = extract_topic(input_text, full_response)
            if topic and topic not in st.session_state.topics:
                st.session_state.topics.append(topic)

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã—ã€å†ç”Ÿã™ã‚‹ (ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãŒã‚ªãƒ³ã®å ´åˆã®ã¿)
            if st.session_state.enable_tts:
                audio_data = text_to_speech(full_response, st.session_state.voice)
                audio_str = f"data:audio/mpeg;base64,{base64.b64encode(audio_data).decode()}"
                audio_html = f"""
                                <audio autoplay>
                                <source src="{audio_str}" type="audio/mpeg">
                                Your browser does not support the audio element.
                                </audio>
                            """
                st.markdown(audio_html, unsafe_allow_html=True)

        elif audio_bytes:
            audio_transcript = audio_to_text(audio_bytes)
            if audio_transcript:
                st.session_state.messages.append({"role": "user", "content": audio_transcript})
                with st.chat_message("user"):
                    st.markdown(audio_transcript)

                # éå»10å›åˆ†ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
                chat_history = st.session_state.chat_session[-20:]

                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
                output_container = st.empty()

                full_response = get_response(audio_transcript, chat_history, output_container)
                st.session_state.chat_session.append({"role": "user", "content": audio_transcript})
                st.session_state.chat_session.append({"role": "assistant", "content": full_response})

                st.session_state.messages.append({"role": "assistant", "content": full_response})

                # ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
                topic = extract_topic(audio_transcript, full_response)
                if topic and topic not in st.session_state.topics:
                    st.session_state.topics.append(topic)

                # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã—ã€å†ç”Ÿã™ã‚‹ (ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãŒã‚ªãƒ³ã®å ´åˆã®ã¿)
                if st.session_state.enable_tts:
                    audio_data = text_to_speech(full_response, st.session_state.voice)
                    audio_str = f"data:audio/mpeg;base64,{base64.b64encode(audio_data).decode()}"
                    audio_html = f"""
                                    <audio autoplay>
                                    <source src="{audio_str}" type="audio/mpeg">
                                    Your browser does not support the audio element.
                                    </audio>
                                """
                    st.markdown(audio_html, unsafe_allow_html=True)

    elif st.session_state["authentication_status"] is False:
        st.sidebar.error('Username/password is incorrect')

    elif st.session_state["authentication_status"] is None:
        st.sidebar.warning('Please enter your username and password')

if __name__ == "__main__":
    main()