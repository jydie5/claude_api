import streamlit as st
import streamlit_authenticator as stauth
import yaml
from yaml.loader import SafeLoader
import os
import base64
from openai import OpenAI
from audio_recorder_streamlit import audio_recorder
import anthropic
import re

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Anthropicã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
anthropic_client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

# ãƒšãƒ¼ã‚¸åˆæœŸè¨­å®š
st.set_page_config(
    page_title="Integrated AI Chat Client",
    page_icon="ğŸŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
def load_config():
    with open('.config.yaml') as file:
        return yaml.load(file, Loader=SafeLoader)

# èªè¨¼ã®åˆæœŸåŒ–
def init_authenticator(config):
    return stauth.Authenticate(
        config['credentials'],
        config['cookie']['name'],
        config['cookie']['key'],
        config['cookie']['expiry_days'],
    )

# éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°
def audio_to_text(audio_bytes):
    write_audio_file("recorded_audio.wav", audio_bytes)
    transcript = openai_client.audio.transcriptions.create(
        model="whisper-1",
        file=open("recorded_audio.wav", mode="rb"),
    )
    return transcript.text

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›¸ãè¾¼ã‚€é–¢æ•°
def write_audio_file(file_path, audio_bytes):
    with open(file_path, "wb") as audio_file:
        audio_file.write(audio_bytes)

# ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã™ã‚‹é–¢æ•°
def text_to_speech(text, voice):
    response = openai_client.audio.speech.create(
        model="tts-1",
        voice=voice,
        input=text,
        response_format="mp3"
    )
    audio_data = response.content
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ã«MP3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    with open("tts.mp3", "wb") as f:
        f.write(audio_data)
    
    return audio_data

def get_response(prompt, chat_history, output_container):
    response = ""
    
    if st.session_state.api_provider == "OpenAI":
        # OpenAI streaming
        openai_stream = openai_client.chat.completions.create(
            model=st.session_state.model,
            messages=[
                *[
                    {
                        "role": message["role"],
                        "content": message["content"]
                    }
                    for message in chat_history
                ],
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            stream=True,
        )

        for chunk in openai_stream:
            if chunk.choices[0].delta.content is not None:
                response += chunk.choices[0].delta.content
                output_container.markdown(response)
    
    elif st.session_state.api_provider == "Anthropic":
        # Anthropic streaming
        with anthropic_client.messages.stream(
            max_tokens=4096,
            messages=[
                *[
                    {
                        "role": message["role"],
                        "content": message["content"]
                    }
                    for message in chat_history
                ],
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model=st.session_state.model,
        ) as stream:
            for text in stream.text_stream:
                response += text
                output_container.markdown(response)
    
    return response

# ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
def extract_topic(user_input, assistant_response):
    user_words = set(user_input.lower().split())
    assistant_words = set(assistant_response.lower().split())
    common_words = user_words.intersection(assistant_words)
    if common_words:
        topic = max(common_words, key=len)
        return re.sub(r'[^a-zA-Z0-9]+', '_', topic)  # ç‰¹æ®Šæ–‡å­—ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›
    return None

# ä¼šè©±å±¥æ­´ã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°
def save_conversation_history(conversation_history, topic=None):
    if topic:
        file_name = f"{topic}.md"
    else:
        file_name = "conversation.md"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«å
    with open(file_name, "w") as file:
        for message in conversation_history:
            if message["role"] == "user":
                file.write(f"User: {message['content']}\n\n")
            else:
                file.write(f"Assistant: {message['content']}\n\n")
    return file_name

def main():
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    config = load_config()

    # èªè¨¼ã®åˆæœŸåŒ–
    authenticator = init_authenticator(config)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ç”»åƒè¡¨ç¤º
    image_path = "./static/ã¡ã„ã‹ã‚ãƒãƒƒãƒ—ã‚³ãƒ¼ãƒ³_blurred.jpg"
    st.sidebar.image(image_path, use_column_width=True)

    # èªè¨¼ç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        authenticator.login()

    if st.session_state["authentication_status"]:
        st.sidebar.write(f'Login Name *{st.session_state["name"]}*')

        # ãƒ¡ã‚¤ãƒ³ç”»é¢ã§ã‚¢ãƒ—ãƒªã®å®Ÿè£…
        st.title('Integrated AI Chat Client')

        # ã‚¢ãƒ—ãƒªç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼
        with st.sidebar:
            st.markdown("## ãŠã—ã‚ƒã¹ã‚Šæ©Ÿèƒ½")
            if "enable_tts" not in st.session_state:
                st.session_state.enable_tts = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§TTSã‚’ç„¡åŠ¹åŒ–
            enable_tts = st.checkbox("Enable Text-to-Speech", st.session_state.enable_tts)
            st.session_state.enable_tts = enable_tts

            if "voice" not in st.session_state:
                st.session_state.voice = "nova"

            voice = st.selectbox("TTS Voice in OpenAI", ["alloy", "echo", "fable", "onyx", "nova", "shimmer"], index=4)
            st.session_state.voice = voice

            # ãƒ­ãƒ¼ã‚«ãƒ«ã®tts.mp3ã‚’å†ç”Ÿã™ã‚‹ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
            if os.path.exists("tts.mp3"):
                if st.button("ã‚‚ã†ä¸€åº¦ãŠã—ã‚ƒã¹ã‚Š"):
                    audio_file = open("tts.mp3", "rb")
                    audio_bytes = audio_file.read()
                    st.audio(audio_bytes, format="audio/mp3")

            # APIæä¾›å…ƒã¨ãƒ¢ãƒ‡ãƒ«é¸æŠ
            st.markdown("## API Provider and Model Selection")
            api_provider = st.selectbox("Choose API Provider", ["OpenAI", "Anthropic"])
            st.session_state.api_provider = api_provider

            if api_provider == "OpenAI":
                model_options = {
                    "gpt-4o-mini (Fast responses)": "gpt-4o-mini",
                    "gpt-4o (Advanced model)": "gpt-4o",
                }
            else:  # Anthropic
                model_options = {
                    "claude-3-haiku (Fast responses)": "claude-3-haiku-20240307",
                    #"claude-3-sonnet (General AI)": "claude-3-sonnet-20240229",
                    "claude-3-opus (OLD-Advanced model)": "claude-3-opus-20240229",
                    "claude-3-5-sonnet (Latest-Fast)": "claude-3-5-sonnet-20240620"
                }
            
            selected_model = st.selectbox(f"{api_provider} Model Type", list(model_options.keys()))
            st.session_state.model = model_options[selected_model]

            # ä¼šè©±ã‚’ä¿å­˜ã™ã‚‹ãƒœã‚¿ãƒ³
            st.markdown("## ãƒ­ã‚°ä¿å­˜")
            if st.button("ä¼šè©±å±¥æ­´ã‚’ä¿å­˜"):
                topic = extract_topic(st.session_state.messages[-2]["content"], st.session_state.messages[-1]["content"])
                file_name = save_conversation_history(st.session_state.messages, topic)
                with open(file_name, "rb") as file:
                    st.download_button(
                        label="ä¼šè©±å±¥æ­´ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=file,
                        file_name=file_name,
                        mime="text/markdown"
                    )
            st.markdown("## ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ")  
                  
        authenticator.logout('Logout', 'sidebar')

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if "chat_session" not in st.session_state:
            st.session_state.chat_session = []
            st.session_state.messages = []
            st.session_state.enable_tts = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§TTSã‚’ç„¡åŠ¹åŒ–
        if "topics" not in st.session_state:
            st.session_state.topics = []

        # æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("New Chat"):
            st.session_state.chat_session = []
            st.session_state.messages = []
            st.session_state.topics = []

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
        if st.session_state.messages:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
        else:
            st.write("No conversation history.")

        # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
        input_text = st.chat_input("Type your message here...", key="input_text")

        # éŸ³å£°å…¥åŠ›
        audio_bytes = audio_recorder(
            text="<<<Recording ends,click again or remain silent for two seconds.>>>",
            recording_color="#e8b62c",
            neutral_color="#2EF218",
            icon_name="microphone-lines",
            icon_size="4x",
            pause_threshold=2.0,
            sample_rate=41_000
        )

        # å…¥åŠ›å‡¦ç†
        if input_text:
            st.session_state.messages.append({"role": "user", "content": input_text})
            with st.chat_message("user"):
                st.markdown(input_text)

            # éå»20å›åˆ†ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
            chat_history = st.session_state.chat_session[-20:]

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
            output_container = st.empty()

            full_response = get_response(input_text, chat_history, output_container)
            st.session_state.chat_session.append({"role": "user", "content": input_text})
            st.session_state.chat_session.append({"role": "assistant", "content": full_response})

            st.session_state.messages.append({"role": "assistant", "content": full_response})

            # ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
            topic = extract_topic(input_text, full_response)
            if topic and topic not in st.session_state.topics:
                st.session_state.topics.append(topic)

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã—ã€å†ç”Ÿã™ã‚‹ (ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãŒã‚ªãƒ³ã®å ´åˆã®ã¿)
            if st.session_state.enable_tts:
                audio_data = text_to_speech(full_response, st.session_state.voice)
                audio_str = f"data:audio/mpeg;base64,{base64.b64encode(audio_data).decode()}"
                audio_html = f"""
                                <audio autoplay>
                                <source src="{audio_str}" type="audio/mpeg">
                                Your browser does not support the audio element.
                                </audio>
                            """
                st.markdown(audio_html, unsafe_allow_html=True)

        elif audio_bytes:
            audio_transcript = audio_to_text(audio_bytes)
            if audio_transcript:
                st.session_state.messages.append({"role": "user", "content": audio_transcript})
                with st.chat_message("user"):
                    st.markdown(audio_transcript)

                # éå»20å›åˆ†ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—
                chat_history = st.session_state.chat_session[-20:]

                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
                output_container = st.empty()

                full_response = get_response(audio_transcript, chat_history, output_container)
                st.session_state.chat_session.append({"role": "user", "content": audio_transcript})
                st.session_state.chat_session.append({"role": "assistant", "content": full_response})

                st.session_state.messages.append({"role": "assistant", "content": full_response})

                # ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
                topic = extract_topic(audio_transcript, full_response)
                if topic and topic not in st.session_state.topics:
                    st.session_state.topics.append(topic)

                # ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã—ã€å†ç”Ÿã™ã‚‹ (ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãŒã‚ªãƒ³ã®å ´åˆã®ã¿)
                if st.session_state.enable_tts:
                    audio_data = text_to_speech(full_response, st.session_state.voice)
                    audio_str = f"data:audio/mpeg;base64,{base64.b64encode(audio_data).decode()}"
                    audio_html = f"""
                                    <audio autoplay>
                                    <source src="{audio_str}" type="audio/mpeg">
                                    Your browser does not support the audio element.
                                    </audio>
                                """
                    st.markdown(audio_html, unsafe_allow_html=True)

    elif st.session_state["authentication_status"] is False:
        st.sidebar.error('Username/password is incorrect')

    elif st.session_state["authentication_status"] is None:
        st.sidebar.warning('Please enter your username and password')

if __name__ == "__main__":
    main()